{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sent: tensor([6, 3, 2, 2])\n",
      "Output sent: tensor([3, 6, 3, 7])\n",
      "torch.Size([1, 4, 1024])\n",
      "Shape of Decoder output: torch.Size([1, 4, 512])\n",
      "The predicted logits are tensor([[[ 0.1240,  0.9169, -0.4552, -0.4940, -0.4583,  0.1199,  0.1451,\n",
      "          -0.6991],\n",
      "         [ 0.1252,  1.1188, -0.3874, -0.4276, -0.4849,  0.0709,  0.1380,\n",
      "          -0.4977],\n",
      "         [ 0.2904,  1.0713, -0.4033, -0.1648, -0.7008,  0.1705,  0.1184,\n",
      "          -0.5556],\n",
      "         [ 0.1432,  1.1053, -0.3179,  0.0422, -0.7275,  0.1366,  0.0114,\n",
      "          -0.2488]]], grad_fn=<ViewBackward0>)\n",
      "Contribution of each word to the loss: tensor([2.6092, 2.0559, 2.3790, 2.4859], grad_fn=<NllLossBackward0>)\n",
      "Total loss: 2.382481813430786\n",
      "\n",
      "tensor([[-0.4354, -0.1223,  0.7927,  ...,  0.8131, -1.1523,  0.5010],\n",
      "        [-1.0468, -0.3158,  1.9221,  ...,  1.9767, -2.7809,  1.2188],\n",
      "        [-0.2511, -0.0725,  0.4484,  ...,  0.4655, -0.6561,  0.2869],\n",
      "        ...,\n",
      "        [-0.4214, -0.1134,  0.7497,  ...,  0.7770, -1.1006,  0.4786],\n",
      "        [ 0.1053, -0.1963, -1.0891,  ..., -0.9384,  1.2118, -0.5718],\n",
      "        [ 0.8625,  0.9932, -0.8650,  ..., -1.1618,  1.3964, -0.7411]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maheepchaudhary/pytorch/Projects/Transformer-from-Scratch/encoder.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return t.tensor(final_sent, dtype = t.float32)\n",
      "/Users/maheepchaudhary/pytorch/Projects/Transformer-from-Scratch/decoder.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return t.tensor(final_sent, dtype = t.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "\n",
    "from imports import *\n",
    "from encoder import Encoder as encoder\n",
    "from decoder import Decoder as decoder\n",
    "\n",
    "# Taking a simple sent that will be passed into the transformer. \n",
    "sent: t.Tensor = t.randint(0, 8, (4,))\n",
    "out_sent: t.Tensor = t.randint(0, 8, (4,))\n",
    "\n",
    "print(f\"Input sent: {sent}\")\n",
    "print(f\"Output sent: {out_sent}\")\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_heads: int, sent: t.Tensor, out_sent: t.Tensor):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = encoder(num_heads, sent)\n",
    "        self.decoder = decoder(num_heads, out_sent)\n",
    "        self.linear_output = nn.Linear(512, 8)  # 8 is the vocabulary size\n",
    "\n",
    "        for params in self.parameters():\n",
    "            params.requires_grad = True\n",
    "\n",
    "    def forward(self) -> t.Tensor:\n",
    "        encoder_output = self.encoder.forward()\n",
    "        decoder_output = self.decoder.forward(encoder_output) \n",
    "        print(f\"Shape of Decoder output: {decoder_output.shape}\")\n",
    "        output = self.linear_output(decoder_output)\n",
    "        return output\n",
    "    \n",
    "# parser = argparse.ArgumentParser(\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "num_heads = 2\n",
    "\n",
    "model = Transformer(num_heads, sent, out_sent)\n",
    "model.train()\n",
    "predicted_logits = model.forward()\n",
    "\n",
    "print(f\"The predicted logits are {predicted_logits}\")\n",
    "\n",
    "ce_loss_none = nn.CrossEntropyLoss(reduction='none')\n",
    "loss_contributions = ce_loss_none(predicted_logits.view(4, 8), out_sent)\n",
    "print(f\"Contribution of each word to the loss: {loss_contributions}\")\n",
    "\n",
    "loss = loss_contributions.mean()\n",
    "print(f\"Total loss: {loss}\"); print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_contributions.backward(t.tensor([1, 1, 1, 1], dtype=t.float32))\n",
    "print(model.linear_output.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_contributions.backward(t.tensor([1, 1, 1, 1], dtype=t.float32))\n",
    "print(model.linear_output.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_contributions.backward(t.tensor([1, 1, 1, 1], dtype=t.float32))\n",
    "print(model.linear_output.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_contributions.backward(t.tensor([1, 1, 1, 1], dtype=t.float32))\n",
    "print(model.linear_output.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (W_q): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_k): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_v): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_o): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (W_q): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_k): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_v): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_q_m): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_k_m): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_v_m): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_o): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (W_o_m): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (linear_output): Linear(in_features=512, out_features=8, bias=True)\n",
      ")\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "GradientEdge(node=<AccumulateGrad object at 0x12ab6ab90>, output_nr=0)\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Task: TO analyse the gradient for the decoder part flowing in each component. \n",
    "'''\n",
    "\n",
    "print(model)\n",
    "\n",
    "print(model.linear_output.weight.grad.size())\n",
    "p = t.autograd.grad(loss, model.linear_output.weight, retain_graph=True)\n",
    "print(p[0].size())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
