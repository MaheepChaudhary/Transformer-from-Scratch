{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¶ External code for the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import types\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def extend(model, input_shape):\n",
    "    if not isinstance(model, nn.Module):\n",
    "        raise TypeError(\"model should be a nn.Module\")\n",
    "    if not isinstance(input_shape, tuple):\n",
    "        raise TypeError(\"input_shape should be a tuple\")\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    weight_input_list = []\n",
    "    weight_output_list = []\n",
    "    weight_repeat_list = []\n",
    "    bias_output_list = []\n",
    "    bias_repeat_list = []\n",
    "\n",
    "    x = torch.zeros((1,) + input_shape, device=device)\n",
    "    with torch.no_grad():\n",
    "        for module in model.children():\n",
    "            y = module(x)\n",
    "            print(y)\n",
    "            if sum(p.numel() for p in module.parameters()):\n",
    "                # for all layers with parameters\n",
    "\n",
    "                # store parameters and clear bias for future calculation\n",
    "                if module.weight is not None:\n",
    "                    initial_weight = module.weight.data.clone()\n",
    "                if module.bias is not None:\n",
    "                    initial_bias = module.bias.data.clone()\n",
    "                    module.bias.data = torch.zeros_like(module.bias)\n",
    "\n",
    "                if module.weight is not None:\n",
    "                    Nweight = module.weight.numel()\n",
    "                    weight_input = []\n",
    "                    weight_output = []\n",
    "                    weight_repeat = torch.zeros(\n",
    "                        Nweight, dtype=torch.long, device=device\n",
    "                    )\n",
    "                    Xeye = torch.eye(x.numel(), device=device).reshape(\n",
    "                        (-1,) + x.shape[1:]\n",
    "                    )\n",
    "                    for i in range(Nweight):\n",
    "                        weight = torch.zeros(Nweight, device=device)\n",
    "                        weight[i] = 1.0\n",
    "                        module.weight.data = weight.reshape(module.weight.shape)\n",
    "                        # output of module is of dimension (j,k)\n",
    "                        out = module(Xeye).reshape(x.numel(), y.numel())\n",
    "                        if (out[out.abs() > 1e-5] - 1.0).abs().max() > 1e-5:\n",
    "                            raise RuntimeError(\n",
    "                                \"the network is not written in the standard form, see https://github.com/ChenAo-Phys/pytorch-Jacobian\"\n",
    "                            )\n",
    "                        nonzero = torch.nonzero(out > 0.5, as_tuple=False)\n",
    "                        weight_input.append(nonzero[:, 0])\n",
    "                        weight_output.append(nonzero[:, 1])\n",
    "                        weight_repeat[i] = nonzero.shape[0]\n",
    "                    weight_input_list.append(torch.cat(weight_input, dim=0))\n",
    "                    weight_output_list.append(torch.cat(weight_output, dim=0))\n",
    "                    weight_repeat_list.append(weight_repeat)\n",
    "                    module.weight.data = initial_weight\n",
    "                else:\n",
    "                    weight_input_list.append(None)\n",
    "                    weight_output_list.append(None)\n",
    "                    weight_repeat_list.append(None)\n",
    "\n",
    "                if module.bias is not None:\n",
    "                    Nbias = module.bias.numel()\n",
    "                    bias_output = []\n",
    "                    bias_repeat = torch.zeros(Nbias, dtype=torch.long, device=device)\n",
    "                    for i in range(Nbias):\n",
    "                        bias = torch.zeros(Nbias, device=device)\n",
    "                        bias[i] = 1.0\n",
    "                        module.bias.data = bias.reshape(module.bias.shape)\n",
    "                        out = module(x).reshape(-1)\n",
    "                        if (out[out.abs() > 1e-5] - 1.0).abs().max() > 1e-5:\n",
    "                            raise RuntimeError(\n",
    "                                \"the network is not written in the standard form, see https://github.com/ChenAo-Phys/pytorch-Jacobian\"\n",
    "                            )\n",
    "                        nonzero = torch.nonzero(out > 0.5, as_tuple=False)\n",
    "                        bias_output.append(nonzero[:, 0])\n",
    "                        bias_repeat[i] = nonzero.shape[0]\n",
    "                    bias_output_list.append(torch.cat(bias_output, dim=0))\n",
    "                    bias_repeat_list.append(bias_repeat)\n",
    "                    module.bias.data = initial_bias\n",
    "                else:\n",
    "                    bias_output_list.append(None)\n",
    "                    bias_repeat_list.append(None)\n",
    "\n",
    "            x = torch.zeros_like(y)\n",
    "\n",
    "    if not hasattr(model, \"_Jacobian_shape_dict\"):\n",
    "        model._Jacobian_shape_dict = {}\n",
    "    model._Jacobian_shape_dict[input_shape] = (\n",
    "        weight_input_list,\n",
    "        weight_output_list,\n",
    "        weight_repeat_list,\n",
    "        bias_output_list,\n",
    "        bias_repeat_list,\n",
    "    )\n",
    "\n",
    "    # assign jacobian method to model\n",
    "    def jacobian(self, as_tuple=False):\n",
    "        shape = self.input_shape\n",
    "        if hasattr(self, \"_Jacobian_shape_dict\") and shape in self._Jacobian_shape_dict:\n",
    "            (\n",
    "                weight_input_list,\n",
    "                weight_output_list,\n",
    "                weight_repeat_list,\n",
    "                bias_output_list,\n",
    "                bias_repeat_list,\n",
    "            ) = self._Jacobian_shape_dict[shape]\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"model or specific input shape is not extended for jacobian calculation\"\n",
    "            )\n",
    "\n",
    "        device = next(model.parameters()).device\n",
    "        jac = []\n",
    "        layer = 0\n",
    "        for module in self.children():\n",
    "            if sum(p.numel() for p in module.parameters()):\n",
    "                weight_input = weight_input_list[layer]\n",
    "                weight_output = weight_output_list[layer]\n",
    "                weight_repeat = weight_repeat_list[layer]\n",
    "                bias_output = bias_output_list[layer]\n",
    "                bias_repeat = bias_repeat_list[layer]\n",
    "                x = self.x_in[layer]\n",
    "                N = x.shape[0]\n",
    "                dz_dy = self.gradient[layer].reshape(N, -1)\n",
    "\n",
    "                if weight_repeat is not None:\n",
    "                    Nweight = weight_repeat.shape[0]\n",
    "                    dz_dy_select = dz_dy[:, weight_output]\n",
    "                    x_select = x.reshape(N, -1)[:, weight_input]\n",
    "                    repeat = torch.repeat_interleave(weight_repeat)\n",
    "                    dz_dW = torch.zeros(N, Nweight, device=device).index_add_(\n",
    "                        1, repeat, dz_dy_select * x_select\n",
    "                    )\n",
    "                    if as_tuple:\n",
    "                        dz_dW = dz_dW.reshape((N,) + module.weight.shape)\n",
    "                    jac.append(dz_dW)\n",
    "                if bias_repeat is not None:\n",
    "                    Nbias = bias_repeat.shape[0]\n",
    "                    dz_dy_select = dz_dy[:, bias_output]\n",
    "                    repeat = torch.repeat_interleave(bias_repeat)\n",
    "                    dz_db = torch.zeros(N, Nbias, device=device).index_add_(\n",
    "                        1, repeat, dz_dy_select\n",
    "                    )\n",
    "                    if as_tuple:\n",
    "                        dz_db = dz_db.reshape((N,) + module.bias.shape)\n",
    "                    jac.append(dz_db)\n",
    "                layer += 1\n",
    "\n",
    "        if as_tuple:\n",
    "            return tuple(jac)\n",
    "        else:\n",
    "            return torch.cat(jac, dim=1)\n",
    "\n",
    "    if not hasattr(model, \"jacobian\"):\n",
    "        model.jacobian = types.MethodType(jacobian, model)\n",
    "\n",
    "\n",
    "class JacobianMode:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        if not isinstance(model, nn.Module):\n",
    "            raise TypeError(\"model should be a nn.Module\")\n",
    "\n",
    "    def __enter__(self):\n",
    "        model = self.model\n",
    "        model.x_in = []\n",
    "        model.gradient = []\n",
    "        self.forward_pre_hook = []\n",
    "        self.backward_hook = []\n",
    "\n",
    "        def record_input_shape(self, input):\n",
    "            model.input_shape = input[0].shape[1:]\n",
    "\n",
    "        def record_forward(self, input, layer):\n",
    "            model.x_in[layer] = input[0].detach()\n",
    "\n",
    "        def record_backward(self, grad_input, grad_output, layer):\n",
    "            model.gradient[layer] = grad_output[0]\n",
    "\n",
    "        module0 = next(model.children())\n",
    "        self.first_forward_hook = module0.register_forward_pre_hook(record_input_shape)\n",
    "\n",
    "        layer = 0\n",
    "        for module in model.children():\n",
    "            if sum(p.numel() for p in module.parameters()):\n",
    "                model.x_in.append(None)\n",
    "                model.gradient.append(None)\n",
    "                self.forward_pre_hook.append(\n",
    "                    module.register_forward_pre_hook(\n",
    "                        partial(record_forward, layer=layer)\n",
    "                    )\n",
    "                )\n",
    "                self.backward_hook.append(\n",
    "                    module.register_backward_hook(partial(record_backward, layer=layer))\n",
    "                )\n",
    "                layer += 1\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.first_forward_hook.remove()\n",
    "        for hook in self.forward_pre_hook:\n",
    "            hook.remove()\n",
    "        for hook in self.backward_hook:\n",
    "            hook.remove()\n",
    "\n",
    "        del self.model.input_shape\n",
    "        del self.model.x_in\n",
    "        del self.model.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[ 0.0081,  0.0952, -0.0858, -0.3452, -0.2276,  0.1357, -0.1907,  0.1563,\n",
      "         -0.1449, -0.0789, -0.1447,  0.2744, -0.1838, -0.2718, -0.0068,  0.2000,\n",
      "          0.2298,  0.0653, -0.2904, -0.0504,  0.0972, -0.2140,  0.0330,  0.0193,\n",
      "          0.1591, -0.2106, -0.3524, -0.0752, -0.1981,  0.1219,  0.1868,  0.1009,\n",
      "          0.1255, -0.1599, -0.0662,  0.0713,  0.1353, -0.2805,  0.0187, -0.2937,\n",
      "         -0.3135,  0.0054,  0.0703,  0.0291, -0.0976, -0.0992, -0.1811,  0.1716,\n",
      "         -0.3285,  0.3295,  0.2083, -0.3178, -0.2265,  0.1358, -0.2843, -0.2679,\n",
      "          0.0066,  0.0384,  0.0302, -0.1789,  0.0421, -0.1842, -0.3064,  0.0110]])\n",
      "tensor([[ 0.0690,  0.0086, -0.0056,  0.0820, -0.1050]])\n",
      "The jacobian shape is torch.Size([2, 901])\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_shape[1] * input_shape[2], 64)\n",
    "        self.fc3 = nn.Linear(64, output_shape[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "input_shape = (1, 2, 4)\n",
    "output_shape = (1, 5)\n",
    "net = MLP(input_shape, output_shape)\n",
    "\n",
    "# Example input\n",
    "example_input = torch.randn(input_shape)\n",
    "\n",
    "extend(net, (1,2,4))\n",
    "\n",
    "x = torch.randn(2,1,2,4)\n",
    "# output shape will be (1000,)\n",
    "\n",
    "# Jacobian computed by the improved method\n",
    "# On Colab CPU 0.16s, K80 GPU 0.14s\n",
    "with JacobianMode(net):\n",
    "    out = net(x)\n",
    "    out.sum().backward()\n",
    "    jac = net.jacobian()\n",
    "    \n",
    "print(f\"The jacobian shape is {jac.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sent: tensor([5, 3, 6, 0])\n",
      "Output sent: tensor([4, 2, 3, 5])\n",
      "torch.Size([1, 4, 1024])\n",
      "Shape of Decoder output: torch.Size([1, 4, 512])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Encoder.forward() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     43\u001b[0m predicted_logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward()\n\u001b[0;32m---> 45\u001b[0m \u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe predicted logits are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_logits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m ce_loss_none \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[55], line 24\u001b[0m, in \u001b[0;36mextend\u001b[0;34m(model, input_shape)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m---> 24\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(y)\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mparameters()):\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;66;03m# for all layers with parameters\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m             \u001b[38;5;66;03m# store parameters and clear bias for future calculation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/transit/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/transit/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Encoder.forward() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from imports import *\n",
    "from encoder import Encoder as encoder\n",
    "from decoder import Decoder as decoder\n",
    "\n",
    "# Taking a simple sent that will be passed into the transformer. \n",
    "sent: t.Tensor = t.randint(0, 8, (4,))\n",
    "out_sent: t.Tensor = t.randint(0, 8, (4,))\n",
    "\n",
    "print(f\"Input sent: {sent}\")\n",
    "print(f\"Output sent: {out_sent}\")\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_heads: int, sent: t.Tensor, out_sent: t.Tensor):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = encoder(num_heads, sent)\n",
    "        self.decoder = decoder(num_heads, out_sent)\n",
    "        self.linear_output = nn.Linear(512, 8)  # 8 is the vocabulary size\n",
    "\n",
    "        for params in self.parameters():\n",
    "            params.requires_grad = True\n",
    "\n",
    "    def forward(self) -> t.Tensor:\n",
    "        encoder_output = self.encoder.forward()\n",
    "        decoder_output = self.decoder.forward(encoder_output) \n",
    "        print(f\"Shape of Decoder output: {decoder_output.shape}\")\n",
    "        output = self.linear_output(decoder_output)\n",
    "        return output\n",
    "    \n",
    "# parser = argparse.ArgumentParser(\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "num_heads = 2\n",
    "\n",
    "model = Transformer(num_heads, sent, out_sent)\n",
    "model.train()\n",
    "predicted_logits = model.forward()\n",
    "\n",
    "extend(model, (4,))\n",
    "\n",
    "print(f\"The predicted logits are {predicted_logits}\")\n",
    "\n",
    "ce_loss_none = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "loss_contributions = ce_loss_none(predicted_logits.view(4, 8), out_sent)\n",
    "print(f\"Contribution of each word to the loss: {loss_contributions}\")\n",
    "\n",
    "loss = loss_contributions.sum()\n",
    "print(f\"Total loss: {loss}\"); print()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=512, out_features=1024, bias=True)\n",
      "tensor([[ 0.0369,  0.5054,  0.4491,  ..., -0.2467,  0.1148, -0.4064],\n",
      "        [ 0.4862, -0.0840,  0.9288,  ...,  0.2381, -0.2027,  0.7542],\n",
      "        [ 0.4276, -0.3685,  0.2085,  ..., -0.2436, -0.5113,  0.5940],\n",
      "        [ 0.9637,  1.6171,  0.2466,  ...,  0.0250,  0.0532, -0.9362]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Linear(in_features=512, out_features=1024, bias=True)\n",
      "tensor([[-0.7159,  0.1916, -0.1559,  ..., -0.6256, -0.3449,  0.2954],\n",
      "        [ 0.1083,  0.3251, -0.3764,  ..., -0.5664,  1.1568,  0.1296],\n",
      "        [ 0.1103,  0.5370, -0.1760,  ...,  0.1072, -0.4360,  0.0615],\n",
      "        [ 0.0761, -0.7269, -0.2071,  ...,  0.7534, -1.1132, -0.6404]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Linear(in_features=512, out_features=1024, bias=True)\n",
      "tensor([[ 0.2403,  0.1211,  0.6501,  ...,  0.1675,  0.4466,  0.6892],\n",
      "        [-0.8572, -0.4634, -0.0071,  ..., -0.7419, -0.4293,  1.0364],\n",
      "        [ 0.4096,  0.5001, -0.0366,  ..., -0.4972,  0.2767, -0.3778],\n",
      "        [ 0.3726,  0.0537, -0.4747,  ..., -0.2158, -0.3927,  1.6568]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Linear(in_features=1024, out_features=512, bias=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x512 and 1024x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/transit/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/transit/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/transit/lib/python3.11/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x512 and 1024x512)"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 512)\n",
    "\n",
    "for module in model.children():\n",
    "    for m in module.children():\n",
    "        print(m)\n",
    "        print(m(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ—¿ Custom Code for the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sent: tensor([0, 0, 2, 6])\n",
      "Output sent: tensor([2, 5, 3, 4])\n",
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (W_q): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_k): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_v): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_o): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (W_q): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_k): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_v): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_q_m): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_k_m): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_v_m): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_o): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (W_o_m): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (linear_output): Linear(in_features=512, out_features=8, bias=True)\n",
      ")\n",
      "torch.Size([1, 4, 1024])\n",
      "Shape of Decoder output: torch.Size([1, 4, 512])\n",
      "The predicted logits are tensor([[[-0.3750,  0.0493, -0.5268,  0.1741, -0.2855, -1.3055,  0.2123,\n",
      "           0.7474],\n",
      "         [-0.4202,  0.1051, -0.2806,  0.1081, -0.3224, -1.1949,  0.3001,\n",
      "           0.5909],\n",
      "         [-0.0719,  0.1000, -0.4076,  0.1813, -0.2907, -1.0663, -0.0556,\n",
      "           0.3868],\n",
      "         [-0.2034,  0.1047, -0.4689,  0.1379, -0.4451, -1.1975, -0.0525,\n",
      "           0.4295]]], grad_fn=<ViewBackward0>)\n",
      "Contribution of each word to the loss: tensor([2.5923, 3.2516, 1.8215, 2.4091], grad_fn=<NllLossBackward0>)\n",
      "Total loss: 10.0745210647583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sys.path.append(\"/Users/maheepchaudhary/pytorch/Projects/Transformer-from-Scratch/\")\n",
    "\n",
    "from encoder import Encoder as encoder\n",
    "from decoder import Decoder as decoder\n",
    "\n",
    "# Taking a simple sent that will be passed into the transformer. \n",
    "sent: t.Tensor = t.randint(0, 8, (4,))\n",
    "out_sent: t.Tensor = t.randint(0, 8, (4,))\n",
    "\n",
    "print(f\"Input sent: {sent}\")\n",
    "print(f\"Output sent: {out_sent}\")\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_heads: int, sent: t.Tensor, out_sent: t.Tensor):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = encoder(num_heads, sent)\n",
    "        self.decoder = decoder(num_heads, out_sent)\n",
    "        self.linear_output = nn.Linear(512, 8)  # 8 is the vocabulary size\n",
    "\n",
    "        for params in self.parameters():\n",
    "            params.requires_grad = True\n",
    "\n",
    "    def forward(self) -> t.Tensor:\n",
    "        encoder_output = self.encoder.forward()\n",
    "        decoder_output = self.decoder.forward(encoder_output) \n",
    "        print(f\"Shape of Decoder output: {decoder_output.shape}\")\n",
    "        output = self.linear_output(decoder_output)\n",
    "        return output\n",
    "    \n",
    "# parser = argparse.ArgumentParser(\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "num_heads = 2\n",
    "\n",
    "model = Transformer(num_heads, sent, out_sent)\n",
    "print(model)\n",
    "model.train()\n",
    "predicted_logits = model.forward()\n",
    "\n",
    "print(f\"The predicted logits are {predicted_logits}\")\n",
    "\n",
    "ce_loss_none = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "loss_contributions = ce_loss_none(predicted_logits.view(4, 8), out_sent)\n",
    "print(f\"Contribution of each word to the loss: {loss_contributions}\")\n",
    "\n",
    "loss = loss_contributions.sum()\n",
    "print(f\"Total loss: {loss}\"); print()\n",
    "\n",
    "# loss_contributions.backward(t.tensor([1, 1, 1, 1], dtype=t.float32), retain_graph=True)\n",
    "# total  = model.linear_output.weight.grad\n",
    "# model.zero_grad()\n",
    "\n",
    "# loss_contributions.backward(t.tensor([1, 0, 0, 0], dtype=t.float32), retain_graph=True)\n",
    "# first_token  = model.linear_output.weight.grad\n",
    "# model.zero_grad()\n",
    "\n",
    "# loss_contributions.backward(t.tensor([0, 1, 0, 0], dtype=t.float32), retain_graph=True)\n",
    "# second_token  = model.linear_output.weight.grad\n",
    "# model.zero_grad()\n",
    "\n",
    "# loss_contributions.backward(t.tensor([0, 0, 1, 0], dtype=t.float32), retain_graph=True)\n",
    "# third_token  = model.linear_output.weight.grad\n",
    "# model.zero_grad()\n",
    "\n",
    "# loss_contributions.backward(t.tensor([0, 0, 0, 1], dtype=t.float32), retain_graph=True)\n",
    "# fourth_token  = model.linear_output.weight.grad\n",
    "# model.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (W_q): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_k): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_v): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_o): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (W_q): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_k): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_v): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_q_m): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_k_m): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_v_m): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (W_o): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (W_o_m): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (linear_output): Linear(in_features=512, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of gradient of last layer is tensor([[ 0.3603, -0.4221, -0.1045,  ..., -0.2058,  0.4342, -0.4643],\n",
      "        [ 0.5072, -0.5805, -0.1513,  ..., -0.2974,  0.6041, -0.6573],\n",
      "        [-0.3724, -0.0117,  0.5763,  ...,  0.3500, -0.5062,  0.6956],\n",
      "        ...,\n",
      "        [-1.0166,  0.7122, -0.0242,  ...,  0.6126, -0.7392,  1.0038],\n",
      "        [ 0.5224, -0.5556, -0.1586,  ..., -0.3127,  0.6003, -0.6696],\n",
      "        [ 0.7897, -0.8532, -0.2648,  ..., -0.4762,  0.9307, -1.0331]])\n",
      "\n",
      "The shape of the gradient of predicted logits is torch.Size([512, 1024])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[8, 4, 1]' is invalid for input of size 524288",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m A \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mpinv(predicted_logits_grad)\n\u001b[0;32m---> 13\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjusting the shape of A to (4, 8, 1)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Adjusting the shape of last_layer to (4, 1, 512) since 4 * 1 * 512 = 2048\u001b[39;00m\n\u001b[1;32m     16\u001b[0m last_layer \u001b[38;5;241m=\u001b[39m last_layer\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m512\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[8, 4, 1]' is invalid for input of size 524288"
     ]
    }
   ],
   "source": [
    "loss.backward(retain_graph=True)\n",
    "last_layer = model.linear_output.weight.grad\n",
    "print(f\"The shape of gradient of last layer is {last_layer}\")\n",
    "print()\n",
    "model.zero_grad()\n",
    "\n",
    "\n",
    "predicted_logits_grad = t.autograd.grad(loss, model.decoder.fc2.weight, retain_graph=True, allow_unused=True)[0]\n",
    "print(f\"The shape of the gradient of predicted logits is {predicted_logits_grad.shape}\")\n",
    "model.zero_grad()\n",
    "\n",
    "A = t.linalg.pinv(predicted_logits_grad)\n",
    "A = A.view(8, 4, 1)  # Adjusting the shape of A to (4, 8, 1)\n",
    "\n",
    "# Adjusting the shape of last_layer to (4, 1, 512) since 4 * 1 * 512 = 2048\n",
    "last_layer = last_layer.view(8, 1, 512)\n",
    "\n",
    "C = t.matmul(A, last_layer)  # Performing the matrix multiplication\n",
    "\n",
    "# Ensuring the resultant shape is (4, 8, 512)\n",
    "C = C.view(4, 8, 512)\n",
    "\n",
    "print(f\"The shape of the resultant matrix C is {C.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
